{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- -----------\n",
      "asttokens         2.4.1\n",
      "chardet           5.2.0\n",
      "colorama          0.4.6\n",
      "comm              0.2.2\n",
      "contourpy         1.2.0\n",
      "cycler            0.12.1\n",
      "debugpy           1.8.1\n",
      "decorator         5.1.1\n",
      "executing         2.0.1\n",
      "fonttools         4.50.0\n",
      "ipykernel         6.29.3\n",
      "ipython           8.22.2\n",
      "jedi              0.19.1\n",
      "joblib            1.3.2\n",
      "jupyter_client    8.6.1\n",
      "jupyter_core      5.7.2\n",
      "kiwisolver        1.4.5\n",
      "matplotlib        3.8.3\n",
      "matplotlib-inline 0.1.6\n",
      "nest-asyncio      1.6.0\n",
      "networkx          3.2.1\n",
      "numpy             1.26.4\n",
      "packaging         24.0\n",
      "pandas            2.2.1\n",
      "parso             0.8.3\n",
      "pillow            10.2.0\n",
      "pip               24.2\n",
      "platformdirs      4.2.0\n",
      "plotly            5.20.0\n",
      "prompt-toolkit    3.0.43\n",
      "psutil            5.9.8\n",
      "pure-eval         0.2.2\n",
      "Pygments          2.17.2\n",
      "pyparsing         3.1.2\n",
      "python-dateutil   2.9.0.post0\n",
      "pytz              2024.1\n",
      "pywin32           306\n",
      "pyzmq             25.1.2\n",
      "reportlab         4.2.0\n",
      "scikit-learn      1.4.1.post1\n",
      "scipy             1.12.0\n",
      "seaborn           0.13.2\n",
      "shapely           2.0.3\n",
      "six               1.16.0\n",
      "stack-data        0.6.3\n",
      "tenacity          8.2.3\n",
      "threadpoolctl     3.4.0\n",
      "tornado           6.4\n",
      "traitlets         5.14.2\n",
      "tzdata            2024.1\n",
      "wcwidth           0.2.13\n"
     ]
    }
   ],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for running module : 0.003 sec.\n"
     ]
    }
   ],
   "source": [
    "# DEZE STAP OVERSLAAN ALS JE ZELF EEN DATASET HEBT. DIT IS OM TE TESTEN MET EEN RANDOM DATASET\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "v220316\n",
    "FUNCTIONALITY\n",
    "    DATASETS: Automatically creates a random dataset of (moving) points in the defaults format\n",
    "EXPLANATION\n",
    "    Creates a random dataset of (moving) points. The number of configurations, time stamps, points and dimensions have to\n",
    "    be entered via the input parameters in the beginning of the code\n",
    "    The default format looks like:\n",
    "    \"con\", \"tst\", \"poi\", \"dim1\" (, \"dim2\", ...)\n",
    "    ...\n",
    "INPUT\n",
    "    Parameters of the dataset to be imported\n",
    "    Parameters\n",
    "        number of configurations (con)\n",
    "        number of time stamps (tst)\n",
    "        number of points (poi)\n",
    "        number of dimensions (dim)\n",
    "OUTPUT\n",
    "    csv-file \"N_C_Dataset.csv\" containing the random dataset of moving objects\n",
    "INPUT PARAMETERS:\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Read parameters from environment variables\n",
    "con = int(os.environ.get('CON', 12))   # number of configurations\n",
    "tst = int(os.environ.get('TST', 2))   # number of time stamps\n",
    "poi = int(os.environ.get('POI', 3))   # number of points\n",
    "dim = int(os.environ.get('DIM', 2))   # number of descriptors/dimensions\n",
    "\n",
    "# Start time\n",
    "t_start = time.time()\n",
    "\n",
    "\n",
    "# Generate random dataset\n",
    "L_rows = []   # create empty row\n",
    "for c in range(con):   # for each configuration\n",
    "    for t in range(tst):   # for each time stamp\n",
    "        for p in range(poi):   # for each point\n",
    "            L_data_arr = [0] * dim\n",
    "            L_data_arr[0] = round(random.uniform(0, 50),2)\n",
    "            L_data_arr[1] = round(random.uniform(0, 50),2)\n",
    "\n",
    "            #for d in range(dim):   # for each descriptor/dimension\n",
    "            #    random_data = random.randrange(100)   # generate random number between 0 and 100\n",
    "            #    L_data_arr[d] = random_data\n",
    "            L_rows.append([c, t, p] + L_data_arr)\n",
    "\n",
    "# Generate meaningful file name\n",
    "file_name = f\"N_C_Dataset_{con}_con_{tst}_tst_{poi}_poi_{dim}_dim.csv\"\n",
    "\n",
    "# Write generated dataset in file\n",
    "with open(file_name, 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    for L_row in L_rows:\n",
    "        wr.writerow(L_row)\n",
    "\n",
    "# Generate meaningful file name\n",
    "file_name = \"N_C_Dataset.csv\"\n",
    "\n",
    "# Write generated dataset in file\n",
    "with open(file_name, 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    for L_row in L_rows:\n",
    "        wr.writerow(L_row)\n",
    "\n",
    "# End and print time\n",
    "print(f'Time elapsed for running module : {round((time.time() - t_start), 3)} sec.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: reportlab in c:\\users\\jmverdoo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.2.0)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\jmverdoo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from reportlab) (10.2.0)\n",
      "Requirement already satisfied: chardet in c:\\users\\jmverdoo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from reportlab) (5.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for running module \"av\": 0.020 sec.\n",
      "Time elapsed for running module \"N_VA_StaticAbsolute\": 4.051 sec.\n",
      "Time elapsed for running module \"N_PDP\": 27.367 sec.\n",
      "Time elapsed for running module \"N_VA_HeatMap\": 2.243 sec.\n",
      "Time elapsed for running module \"N_VA_HClust\": 0.264 sec.\n",
      "Time elapsed for running module \"N_VA_Mds\": 0.353 sec.\n",
      "Time elapsed for running module \"N_VA_TopK\": 5.970 sec.\n",
      "Time elapsed for running module \"N_T_Report\": 16.359 sec.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'N_C_PDPg_buffer_Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 235\u001b[0m\n\u001b[0;32m    232\u001b[0m av\u001b[38;5;241m.\u001b[39mdataset_name_exclusive \u001b[38;5;241m=\u001b[39m av\u001b[38;5;241m.\u001b[39mdataset_name[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]  \u001b[38;5;66;03m# The dataset without the last four characters \".csv\"\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Open the buffered file as a dataframe with a header in the current working directory\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m av\u001b[38;5;241m.\u001b[39mDf_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m av\u001b[38;5;241m.\u001b[39mDf_dataset\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtstID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoiID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# Open the file as a list\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'N_C_PDPg_buffer_Dataset.csv'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 6000x4500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#N_Moving_Objects uitvoeren en bugs fixen. Wat ik niet direct kan oplossen even afzetten en dat nadien dan oplossen\n",
    "#N_VA_DynamicAbsolute is afgezet omdat het nog een fout gaf. Dit nog aanpassen.\n",
    "#N_VA_ClusterMap is afgezet omdat het een fout gaf. Dit nog aanpassen.\n",
    "#het printen van het aantal \"arrays/interacties\" afzetten, maar dit wel opslaan en in het Report tonen.\n",
    "\n",
    "#met Rough en buffer spelen om daar een goed inzicht in te krijgen wat de afstanden precies bedoelen\n",
    "#daarna naar drie stilstaande punten gaan.\n",
    "#Rough verder uitwerken: werkt nog niet volledig en daarna moet het nog in rapport geplaatst worden ook\n",
    "#!!!in de toekenning van de variabelen in report is er nog een probleem. Dit nog in orde brengen\n",
    "#de variabelen voor de verschillende \"fundamental\" en \"buffer\" zijn nog niet perfect. Nog finetunen.\n",
    "#de buffer berekeningen moeten in rapport komen\n",
    "\n",
    "#make sure ti calculate normal and buffer with a push on the button so that they can be sompared wreach other\n",
    "#create the same things in the document for fuzzy and/or buffer a nd then check. After that do with external point (center of the field); and then do with I object (ball) and with one time stamp; then do with another of external players...\n",
    "#mak sure to always clearly show what isdone with a title.\n",
    "#give each page a page number\n",
    "\n",
    "import av  # Import all variables\n",
    "import csv  # For reading and writing csv files\n",
    "import importlib  # For reloading modules\n",
    "import numpy as np  # For numerical calculations\n",
    "import pandas as pd  # For data manipulation\n",
    "import shutil  # For file handling\n",
    "import time  # For timing the code\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "# Import modules for static and dynamic visualizations based on configuration settings\n",
    "if av.N_VA_StaticAbsolute == 1:\n",
    "    import N_VA_StaticAbsolute  # Creates the absolute (vectors wrt whole region) static visualizations\n",
    "if av.N_VA_StaticRelative == 1:\n",
    "    import N_VA_StaticRelative  # Creates the relative (vectors wrt to 'local maximum') static visualizations\n",
    "if av.N_VA_StaticFinetuned == 1:\n",
    "    import N_VA_StaticFinetuned  # Creates the finetuned static visualizations for a specific case that has to be finetuned here ; might be necessary to fintune some things in this code for the specific case\n",
    "if av.N_VA_DynamicAbsolute == 1:\n",
    "    import N_VA_DynamicAbsolute  # Creates dynamic visualizations. Not ok yet\n",
    "\n",
    "# This function takes the original av_dataset and creates the working dataset from that\n",
    "def SetDataForPDPType(data_filename, D_point_mapping, curr_point_id, window_length_tst):\n",
    "    # Open the file as a dataframe with a header\n",
    "    Df_dataset = pd.read_csv(data_filename, header=None)\n",
    "    Df_dataset.columns = ['conID', 'tstID', 'poiID', 'x', 'y']\n",
    "    # Open the file as a list\n",
    "    L_dataset = []  # Create an empty list\n",
    "    with open(data_filename) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for L_row in csv_reader:\n",
    "            poi_id = L_row[0]\n",
    "            try:\n",
    "                int(poi_id)\n",
    "            except ValueError:\n",
    "                if poi_id not in D_point_mapping:\n",
    "                    D_point_mapping[poi_id] = curr_point_id\n",
    "                    curr_point_id += 1\n",
    "                L_row[2] = D_point_mapping[poi_id]\n",
    "            L_dataset.append(list(map(float, L_row)))\n",
    "    # Transform list to array\n",
    "    A_dataset = np.array(L_dataset, dtype=np.float32)\n",
    "    # Save dataframe \"Df_dataset\"\n",
    "    Df_dataset.to_csv(\"Df_dataset.csv\", index=False)\n",
    "    # Automatically detected variables\n",
    "    # Detect the number of configurations in the dataset\n",
    "    con = Df_dataset['conID'].max() + 1\n",
    "    # Detect the number of time stamps in the dataset\n",
    "    tst = Df_dataset['tstID'].max() + 1\n",
    "    # Detect the number of points in the dataset\n",
    "    poi = Df_dataset['poiID'].max() + 1\n",
    "    # Checks\n",
    "    if av.window_length_tst > tst:\n",
    "        print(\"ERROR IN VALUE OF VARIABLE: window_length_tst > tst\")\n",
    "    return Df_dataset, A_dataset, con, tst, poi\n",
    "\n",
    "if av.PDPg_fundamental == 1:\n",
    "    av.PDPg_fundamental_active = 1\n",
    "    shutil.copyfile(av.dataset_name, \"N_C_PDPg_fundamental_Dataset.csv\")\n",
    "    av.dataset_name = 'N_C_PDPg_fundamental_Dataset.csv'\n",
    "    av.dataset_name_exclusive = av.dataset_name [:-4]\n",
    "    # Open the original file as a dataframe with a header in the current working directory\n",
    "    av.Df_dataset = pd.read_csv(\"N_C_PDPg_fundamental_Dataset.csv\", header=None)\n",
    "    av.Df_dataset.columns = ['conID', 'tstID', 'poiID', 'x', 'y']\n",
    "    # Open the file as a list in the current working directory\n",
    "    av.L_dataset = []  # Create an empty list\n",
    "    with open(av.dataset_name) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for L_row in csv_reader:\n",
    "            poi_id = L_row[0]\n",
    "            #try:\n",
    "            int(poi_id)\n",
    "            av.L_dataset.append(list(map(float, L_row)))\n",
    "    # Transform list to array\n",
    "    av.A_dataset = np.array(av.L_dataset, dtype=np.float32)\n",
    "    # Save dataframe \"Df_dataset\"\n",
    "    av.Df_dataset.to_csv(\"Df_dataset.csv\", index=False)\n",
    "    # Automatically detected variables\n",
    "    # Detect the number of configurations in the dataset\n",
    "    av.con = av.Df_dataset['conID'].max() + 1\n",
    "    # Detect the number of time stamps in the dataset\n",
    "    av.tst = av.Df_dataset['tstID'].max() + 1\n",
    "    # Detect the number of points in the dataset\n",
    "    av.poi = av.Df_dataset['poiID'].max() + 1\n",
    "\n",
    "    if av.N_PDP == 1:\n",
    "         import N_PDP  # Execute PDP\n",
    "    if av.N_VA_HeatMap == 1:\n",
    "         import N_VA_HeatMap  # Creates a heat map, based on the distance matrix\n",
    "    if av.N_VA_HClust == 1:\n",
    "        import N_VA_HClust  # Creates a hierarchical cluster tree, based on the distance matrix\n",
    "    if av.N_VA_ClusterMap == 1:\n",
    "        import N_VA_ClusterMap  # Creates a cluster map, based on the distance matrix\n",
    "    if av.N_VA_Mds == 1:\n",
    "        import N_VA_Mds  # Creates a dimension reduction, based on the distance matrix\n",
    "    if av.N_VA_Mds_autoencoder == 1:\n",
    "        import N_VA_Mds_autoencoder # Creates a dimension reduction, using autoencoder\n",
    "    if av.N_VA_TopK == 1:\n",
    "        import N_VA_TopK  # Creates the topK visualisations\n",
    "    if av.N_VA_Inverse == 1:\n",
    "        import N_VA_Inverse  # Creates similar configurations\n",
    "    if av.N_VA_Report == 1:\n",
    "        import N_T_Report  # Create the report\n",
    "    av.PDPg_fundamental_active = 0\n",
    "\n",
    "if av.PDPg_buffer == 1:\n",
    "    av.PDPg_buffer_active = 1\n",
    "    import N_T_OB\n",
    "    av.dataset_name = 'N_C_PDPg_buffer_Dataset.csv'  # Filename of csv file when buffer and fine borders\n",
    "    av.dataset_name_exclusive = av.dataset_name [:-4] #The dataset without the last four characters \".csv\"\n",
    "    # Open the buffer file as a dataframe with a header in the current working directory\n",
    "    av.Df_dataset = pd.read_csv(\"N_C_PDPg_buffer_Dataset.csv\", header=None)\n",
    "    av.Df_dataset.columns = ['conID', 'tstID', 'poiID', 'x', 'y']\n",
    "    # Open the file as a list in the current working directory\n",
    "    av.L_dataset = []  # Create an empty list\n",
    "    with open(av.dataset_name) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for L_row in csv_reader:\n",
    "            poi_id = L_row[0]\n",
    "            int(float(poi_id))\n",
    "            av.L_dataset.append(list(map(float, L_row)))\n",
    "    # Transform list to array\n",
    "    av.A_dataset = np.array(av.L_dataset, dtype=np.float32)\n",
    "    # Save dataframe \"Df_dataset\"\n",
    "    av.Df_dataset.to_csv(\"Df_dataset.csv\", index=False)\n",
    "    # Automatically detected variables\n",
    "    # Detect the number of configurations in the dataset\n",
    "    av.con = av.Df_dataset['conID'].max() + 1\n",
    "    # Detect the number of time stamps in the dataset\n",
    "    av.tst = av.Df_dataset['tstID'].max() + 1\n",
    "    # Detect the number of points in the dataset\n",
    "    av.poi = av.Df_dataset['poiID'].max() + 1\n",
    "\n",
    "    if av.N_PDP == 1:\n",
    "        importlib.reload(N_PDP)\n",
    "    if av.N_VA_HeatMap == 1:\n",
    "        importlib.reload(N_VA_HeatMap)\n",
    "    if av.N_VA_HClust == 1:\n",
    "        importlib.reload(N_VA_HClust)\n",
    "    if av.N_VA_ClusterMap == 1:\n",
    "        importlib.reload(N_VA_ClusterMap)\n",
    "    if av.N_VA_Mds == 1:\n",
    "        importlib.reload(N_VA_Mds)\n",
    "    if av.N_VA_TopK == 1:\n",
    "        importlib.reload(N_VA_TopK)\n",
    "    if av.N_VA_Inverse == 1:\n",
    "        importlib.reload(N_VA_Inverse)\n",
    "    if av.N_VA_Report == 1:\n",
    "        importlib.reload(N_T_Report)\n",
    "    av.PDPg_buffer_active = 0\n",
    "\n",
    "if av.PDPg_rough == 1:\n",
    "    av.PDPg_rough_active = 1\n",
    "    #import N_T_OR\n",
    "\n",
    "    #shutil.copyfile(av.dataset_name, \"N_C_PDPg_rough_Dataset.csv\")\n",
    "\n",
    "    av.dataset_name = 'N_C_Dataset.csv'  # Filename of csv file when no buffer and rough borders\n",
    "    #av.dataset_name = 'N_C_PDPg_fundamental_Dataset.csv'  # Filename of csv file when no buffer and rough borders; this is just the original file because the roughness is taken into account when the inequality matrix values are calculated.\n",
    "    av.dataset_name_exclusive = av.dataset_name [:-4] # The dataset without the last four characters \".csv\"\n",
    "\n",
    "    # Open the original file as a dataframe with a header in the current working directory\n",
    "    av.Df_dataset = pd.read_csv(\"N_C_PDPg_fundamental_Dataset.csv\", header=None)\n",
    "    #av.Df_dataset = pd.read_csv(\"N_C_Dataset.csv\", header=None)\n",
    "    av.Df_dataset.columns = ['conID', 'tstID', 'poiID', 'x', 'y']\n",
    "    # Open the file as a list in the current working directory\n",
    "    av.L_dataset = []  # Create an empty list\n",
    "    with open(av.dataset_name) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for L_row in csv_reader:\n",
    "            poi_id = L_row[0]\n",
    "            int(float(poi_id))\n",
    "            av.L_dataset.append(list(map(float, L_row)))\n",
    "    # Transform list to array\n",
    "    av.A_dataset = np.array(av.L_dataset, dtype=np.float32)\n",
    "    # Save dataframe \"Df_dataset\"\n",
    "    av.Df_dataset.to_csv(\"Df_dataset.csv\", index=False)\n",
    "    # Automatically detected variables\n",
    "    # Detect the number of configurations in the dataset\n",
    "    av.con = av.Df_dataset['conID'].max() + 1\n",
    "    # Detect the number of time stamps in the dataset\n",
    "    av.tst = av.Df_dataset['tstID'].max() + 1\n",
    "    # Detect the number of points in the dataset\n",
    "    av.poi = av.Df_dataset['poiID'].max() + 1\n",
    "\n",
    "    if av.N_PDP == 1:\n",
    "        importlib.reload(N_PDP)\n",
    "    if av.N_VA_HeatMap == 1:\n",
    "        importlib.reload(N_VA_HeatMap)\n",
    "    if av.N_VA_HClust == 1:\n",
    "        importlib.reload(N_VA_HClust)\n",
    "    if av.N_VA_ClusterMap == 1:\n",
    "        importlib.reload(N_VA_ClusterMap)\n",
    "    if av.N_VA_Mds == 1:\n",
    "        importlib.reload(N_VA_Mds)\n",
    "    if av.N_VA_TopK == 1:\n",
    "        importlib.reload(N_VA_TopK)\n",
    "    if av.N_VA_Inverse == 1:\n",
    "        importlib.reload(N_VA_Inverse)\n",
    "    if av.N_VA_Report == 1:\n",
    "        importlib.reload(N_T_Report)\n",
    "    av.PDPg_rough_active = 0\n",
    "\n",
    "if av.PDPg_bufferrough == 1:\n",
    "    av.PDPg_bufferrough_active = 1\n",
    "    #import N_T_OBR\n",
    "    import N_T_OB\n",
    "    av.dataset_name = 'N_C_PDPg_buffer_Dataset.csv'  # Filename of csv file when buffer\n",
    "    av.dataset_name_exclusive = av.dataset_name [:-4] #The dataset without the last four characters \".csv\"\n",
    "\n",
    "    # Open the rough file as a dataframe with a header in the current working directory\n",
    "    av.Df_dataset = pd.read_csv(\"N_C_PDPg_buffer_Dataset.csv\", header=None)\n",
    "    av.Df_dataset.columns = ['conID', 'tstID', 'poiID', 'x', 'y']\n",
    "    # Open the file as a list in the current working directory\n",
    "    av.L_dataset = []  # Create an empty list\n",
    "    with open(av.dataset_name) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for L_row in csv_reader:\n",
    "            poi_id = L_row[0]\n",
    "            int(float(poi_id))\n",
    "            av.L_dataset.append(list(map(float, L_row)))\n",
    "    # Transform list to array\n",
    "    av.A_dataset = np.array(av.L_dataset, dtype=np.float32)\n",
    "    # Save dataframe \"Df_dataset\"\n",
    "    av.Df_dataset.to_csv(\"Df_dataset.csv\", index=False)\n",
    "    # Automatically detected variables\n",
    "    # Detect the number of configurations in the dataset\n",
    "    av.con = av.Df_dataset['conID'].max() + 1\n",
    "    # Detect the number of time stamps in the dataset\n",
    "    av.tst = av.Df_dataset['tstID'].max() + 1\n",
    "    # Detect the number of points in the dataset\n",
    "    av.poi = av.Df_dataset['poiID'].max() + 1\n",
    "\n",
    "    if av.N_PDP == 1:\n",
    "        importlib.reload(N_PDP)\n",
    "    if av.N_VA_HeatMap == 1:\n",
    "        importlib.reload(N_VA_HeatMap)\n",
    "    if av.N_VA_HClust == 1:\n",
    "        importlib.reload(N_VA_HClust)\n",
    "    if av.N_VA_ClusterMap == 1:\n",
    "        importlib.reload(N_VA_ClusterMap)\n",
    "    if av.N_VA_Mds == 1:\n",
    "        importlib.reload(N_VA_Mds)\n",
    "    if av.N_VA_TopK == 1:\n",
    "        importlib.reload(N_VA_TopK)\n",
    "    if av.N_VA_Inverse == 1:\n",
    "        importlib.reload(N_VA_Inverse)\n",
    "    if av.N_VA_Report == 1:\n",
    "        importlib.reload(N_T_Report)\n",
    "    av.PDPg_bufferrough_active = 0\n",
    "\n",
    "# End and print time\n",
    "print('Time elapsed for running module \"N_Moving_Objects\": {:.3f} sec.'.format(time.time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.36\n",
      "39.03\n",
      "20.42\n",
      "15.17\n",
      "39.65\n",
      "47.52\n",
      "48.84\n",
      "5.47\n",
      "16.73\n",
      "48.04\n",
      "13.86\n",
      "3.82\n",
      "30.45\n",
      "17.69\n",
      "13.42\n",
      "31.82\n",
      "25.26\n",
      "34.78\n",
      "26.53\n",
      "0.68\n",
      "38.64\n",
      "17.9\n",
      "42.81\n",
      "34.76\n",
      "29.98\n",
      "6.56\n",
      "5.49\n",
      "26.95\n",
      "37.99\n",
      "30.79\n",
      "6.73\n",
      "11.7\n",
      "39.1\n",
      "8.4\n",
      "17.2\n",
      "43.03\n",
      "46.31\n",
      "3.46\n",
      "23.56\n",
      "48.39\n",
      "20.22\n",
      "3.4\n",
      "8.64\n",
      "7.03\n",
      "42.04\n",
      "7.4\n",
      "5.72\n",
      "42.21\n",
      "33.29\n",
      "42.02\n",
      "28.99\n",
      "34.44\n",
      "39.12\n",
      "10.35\n",
      "32.66\n",
      "2.9\n",
      "29.77\n",
      "34.96\n",
      "26.58\n",
      "14.05\n",
      "30.92\n",
      "1.9\n",
      "26.1\n",
      "49.3\n",
      "23.18\n",
      "41.96\n",
      "41.56\n",
      "44.85\n",
      "29.34\n",
      "7.51\n",
      "35.9\n",
      "42.88\n",
      "46.13\n",
      "30.39\n",
      "3.48\n",
      "44.32\n",
      "18.56\n",
      "20.74\n",
      "42.15\n",
      "30.31\n",
      "40.12\n",
      "9.66\n",
      "24.12\n",
      "26.89\n",
      "9.9\n",
      "13.4\n",
      "10.22\n",
      "45.35\n",
      "40.39\n",
      "7.79\n",
      "41.3\n",
      "37.37\n",
      "29.9\n",
      "24.45\n",
      "44.41\n",
      "30.17\n",
      "21.94\n",
      "29.61\n",
      "18.09\n",
      "17.3\n",
      "0.58\n",
      "7.46\n",
      "31.71\n",
      "22.0\n",
      "29.21\n",
      "2.9\n",
      "38.33\n",
      "21.94\n",
      "39.14\n",
      "2.64\n",
      "29.01\n",
      "31.93\n",
      "40.25\n",
      "47.94\n",
      "26.26\n",
      "27.13\n",
      "19.38\n",
      "6.03\n",
      "6.86\n",
      "48.37\n",
      "38.75\n",
      "49.16\n",
      "19.85\n",
      "29.05\n",
      "6.3\n",
      "43.56\n",
      "13.88\n",
      "6.77\n",
      "42.76\n",
      "2.64\n",
      "14.18\n",
      "35.6\n",
      "6.27\n",
      "19.68\n",
      "6.72\n",
      "10.68\n",
      "45.0\n",
      "47.1\n",
      "22.43\n",
      "47.93\n",
      "22.86\n",
      "17.68\n",
      "16.9\n",
      "27.58\n",
      "Time elapsed for running module \"N_VA_StaticAbsolute\": 9.235 sec.\n"
     ]
    }
   ],
   "source": [
    "# STATIC VISUALISATION (ARROWS ON PLOT)\n",
    "\n",
    "\n",
    "# Load the necessary libraries\n",
    "from matplotlib import cm  # Colormaps\n",
    "import matplotlib.patches as patches  # For drawing shapes\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import matplotlib as mpl  # Matplotlib settings\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Import custom attributes\n",
    "import av\n",
    "\n",
    "# Start time\n",
    "t_start = time.time()\n",
    "\n",
    "# Set the default unit of length to centimeters\n",
    "mpl.rcParams['figure.dpi'] = 2.54\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(av.dataset_name, header=None)  \n",
    "\n",
    "# Get the unique values of the first column\n",
    "configurations = df[0].unique() \n",
    "\n",
    "# Create a list of colors\n",
    "if av.poi == 3:\n",
    "    colors = ['black', 'blue', 'magenta']\n",
    "else:\n",
    "    colors = [plt.cm.cividis(i/av.poi) for i in range(av.poi)]\n",
    "\n",
    "# Create the scatterplot, including arrows\n",
    "for config in configurations:\n",
    "    config_data = df[df[0] == config]  # Get the data for the current configuration\n",
    "    \n",
    "    # Get the x/y-values\n",
    "    x = config_data[3]\n",
    "    y = config_data[4]\n",
    "    \n",
    "    # Calculate scaling factor\n",
    "    x_range = x.max() - x.min()\n",
    "    y_range = y.max() - y.min()\n",
    "    scaling_factor = min(x_range, y_range) / 10\n",
    "    #plt.figure(figsize=(12, 8))  # Set the size of the figure\n",
    "    plt.figure(figsize=(18, 18))  # Set the size of the figure\n",
    "    \n",
    "    # Set the limits of the axes\n",
    "    plt.xlim(av.min_boundary_x, av.max_boundary_x)\n",
    "    plt.ylim(av.min_boundary_y, av.max_boundary_y)\n",
    "    \n",
    "    # Set the labels of the axes\n",
    "    plt.xlabel('X-Axis (m)', fontsize=30, fontname='monospace')  \n",
    "    plt.ylabel('Y-Axis (m)', fontsize=30, fontname='monospace')\n",
    "    ax = plt.gca()  # Get the current axes\n",
    "    ax.tick_params(axis='both', labelsize=30, labelcolor='black')  # Set the size and color of the tick labels\n",
    "    \n",
    "    # Check if there's only one timestamp\n",
    "    if av.tst == 1:\n",
    "        #colors = ['blue', 'red', 'green', 'yellow', 'orange', 'purple']\n",
    "        #plt.scatter(x, y, color=colors[i % 10], s=100)  # s is the marker size\n",
    "        for point_index, (x_val, y_val) in enumerate(zip(x, y)):\n",
    "            color_to_use = colors[point_index % len(colors)]\n",
    "            plt.scatter(x_val, y_val, color=color_to_use, s=200)  # s is the marker size\n",
    "    else:\n",
    "        vector_index = 0\n",
    "        # Add vectors between points\n",
    "        for p in range(av.poi):  # for each point\n",
    "            for i in range(av.tst - 1):  # for each interval\n",
    "                x1 = x.iloc[p + i * av.poi]\n",
    "                y1 = y.iloc[p + i * av.poi]\n",
    "                x2 = x.iloc[p + i * av.poi + av.poi]\n",
    "                y2 = y.iloc[p + i * av.poi + av.poi]\n",
    "                # Use the custom color list to assign a un@ique color to each point\n",
    "                color_to_use = colors[p % len(colors)]\n",
    "                plt.arrow(x1, y1, x2 - x1, y2 - y1, length_includes_head=True, head_width=(((x.max()+1)-(x.min()-1))/40), head_length=(((x.max()+1)-(x.min()-1))/20), linewidth=10, color=color_to_use)\n",
    "\n",
    "                #print(x1)\n",
    "                #print(x2)\n",
    "                #print(y1)\n",
    "                #print(y2)\n",
    "\n",
    "                \n",
    "                # Add label for the first timestamp of each point\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "                if i == 0:\n",
    "                    plt.gca().text(x1, y1, f'p{p}', fontsize=30, ha='right')\n",
    "                \n",
    "                vector_index += 1\n",
    "    \n",
    "    \"\"\"\n",
    "    # Draw the tennis pitch\n",
    "    plt.hlines(0, -5.485, 5.485, linewidth=0.5, colors='g', linestyles='solid')\n",
    "    plt.hlines(6.4, -4.115, 4.115, linewidth=0.5, colors='g', linestyles='solid')\n",
    "    plt.hlines(-6.4, -4.115, 4.115, linewidth=0.5, colors='g', linestyles='solid')\n",
    "    plt.hlines(11.885, -5.485, 5.485, linewidth=0.5, colors='g', linestyles='solid')\n",
    "    plt.hlines(-11.885, -5.485, 5.485, linewidth=0.5, colors='g', linestyles='solid')\n",
    "    plt.vlines(0, -6.4, 6.4, linewidth=0.5, colors='g', linestyles='solid')\n",
    "    plt.vlines(-5.485, -11.885, 11.885, linewidth=0.5, colors='g', linestyles='solid')\n",
    "    plt.vlines(5.485, -11.885, 11.885, linewidth=0.5, colors='g', linestyles='solid')\n",
    "    plt.vlines(-4.115, -11.885, 11.885, linewidth=0.5, colors='g', linestyles='solid')\n",
    "    plt.vlines(4.115, -11.885, 11.885, linewidth=0.5, colors='g', linestyles='solid')\n",
    "    \"\"\"\n",
    "\n",
    "    plt.title(\"Configuration {}\".format(config),fontname=\"monospace\", fontsize=40)\n",
    "    file_name = \"N_C_Csa{}.png\".format(config) # csa from configuration static absolute    \n",
    "    plt.savefig(file_name, dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # close the figure to release memory\n",
    "\n",
    "\n",
    "\n",
    "# End and print time\n",
    "print('Time elapsed for running module \"N_VA_StaticAbsolute\": {:.3f} sec.'.format(time.time() - t_start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
